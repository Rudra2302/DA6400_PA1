{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import wandb\n",
    "from gym_minigrid.wrappers import FullyObsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a66ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {'method': 'grid'}\n",
    "sweep_config['metric'] = {'name': 'reward', 'goal': 'maximize'}\n",
    "\n",
    "sweep_config['parameters'] = {\n",
    "    'learning_rate': {'values': [0.1, 0.05, 0.01, 0.2, 0.25]},\n",
    "    'episodes': {'values': [10000, 20000, 5000, 30000, 40000]}\n",
    "}\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"RL_PA1_GridWorld_QLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x)) \n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "def find_agent(image):\n",
    "    for x in range(image.shape[0]):\n",
    "        for y in range(image.shape[1]):\n",
    "            if image[x, y, 0] == 10:\n",
    "                return (x, y)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eeea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(config=None):\n",
    "    env = gym.make(\"MiniGrid-Dynamic-Obstacles-5x5-v0\")\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = FullyObsWrapper(env)\n",
    "    discount = 0.99\n",
    "    \n",
    "    with wandb.init(): \n",
    "        config = wandb.config\n",
    "        learning_rate = config.learning_rate\n",
    "        episodes = config.episodes\n",
    "        state_space_size = (5, 5, 4)\n",
    "        q_table = np.random.uniform(low=-2, high=0, size=(state_space_size + (env.action_space.n,)))\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            done = False\n",
    "            obs_dict = env.reset()[0]\n",
    "            step_count = 0\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                agent_pos = find_agent(obs_dict[\"image\"])\n",
    "                agent_dir = obs_dict['direction']  \n",
    "                state = (agent_pos[0], agent_pos[1], agent_dir)\n",
    "                \n",
    "                probabilities = softmax(q_table[state])\n",
    "                action = np.random.choice(env.action_space.n, p=probabilities)\n",
    "                \n",
    "                new_obs_dict = env.step(action)[0]\n",
    "                step_count += 1\n",
    "                \n",
    "                reward = 1 - 0.9 * (step_count / 100)\n",
    "                if done and reward <= 0:\n",
    "                    reward -= 1\n",
    "                total_reward += reward\n",
    "                \n",
    "                new_agent_pos = find_agent(new_obs_dict[\"image\"])\n",
    "                new_agent_dir = new_obs_dict['direction']\n",
    "                new_state = (new_agent_pos[0], new_agent_pos[1], new_agent_dir)\n",
    "                \n",
    "                max_future_q = np.max(q_table[new_state]) if not done else 0\n",
    "                current_q = q_table[state + (action, )]\n",
    "                new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount * max_future_q)\n",
    "                q_table[state + (action, )] = new_q\n",
    "                obs_dict = new_obs_dict\n",
    "                \n",
    "                wandb.log({'reward': total_reward, 'episode': ep})\n",
    "                wandb.log({'length': step_count, 'episode': ep})\n",
    "    \n",
    "        env.close()\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, qlearning, count=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
